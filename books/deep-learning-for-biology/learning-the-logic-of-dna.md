# Learning the Logic of DNA

- Based on a DNA sequence, predict if it's bound by transcrition factors (TFs â€” class of proteins)
  - TFs are related to gene regulation
  - TFs bind to a DNA sequence and make surrounding genes to be turned on or off
- Biology Primer
  - DNA is a molecule of inheritance
  - DNA of a cell has all the instructions to build an entire human body
  - DNA is built from 4 unique letter, or nucleotides (A (adenine), C (cytosine), G (guanine), and T (thymine))
  - It holds processes like cell division and differentiation
  - Genome is a complete set of DNA in an organism
    - Genetics: study specific genes or small set of genes
    - Genomics: study an entire genome
- Coding and non-coding regions
  - Coding regions: DNA that are transcribed into RNA and translated into proteins (DNA -> RNA -> Protein)
  - The human genome contains around 20,000 protein-coding genes.
  - Protein-coding genes account for only about 2% of the genome. The remaining 98% is noncoding DNA.
  - Non-coding DNA doesn't produce proteins but plays a critical regulatory role
    - It can produce RNA that regulate gene expression
    - It can help organize 3D structure of the genome
    - It can serve as docking sites for regulatory proteins
- How Transcription Factors Orchestrate Gene Activity
  - Transcription factors binding means that a TF binds to a specific DNA sequence with the purpose of regulate the gene expression: increase/activate or decrease/repress the rate at which the gene is transcribed into RNA
- CNNs are also used in 1D data like DNA sequences
  - Shallow layers: detect low-level DNA features (GC-rich, AT-rich regions)
  - Mid-level filters: identify known TF motifs
  - Deeper layers: learn higher-level features
  - CNNs are relatively lightweight and easy to train, but struggle with interactions between distant bases, or in the words, problems involving long-range dependencies, relationships between elements far apart in a sequence
- Transformers
  - CNNs are great to find local patterns
  - Transformers are powerful for modeling relationships across long distance in a sequence, and global sequence context
  - Self-attention is a mechanism that receives input token embeddings (input vector) and outputs context-aware embeddings (output vector)
    - It performs this process by making each token attend and check how much it influences other token positions
  - Transformer Block: attention/self attention -> feedforward layer -> residual connection -> layer normalization
  - Transformer models have many of these blocks stacked
  - Multiheaded Attention: it runs several attention mechanisms in parallel to help capture a richer and more diverse set of relationships within the data
- Modeling task
  - Predict if a 200-base DNA sequence, can we predict whether it binds to a specific TF called CTCF
  - It's a binary classification problem
    - Input: DNA sequence
    - Output: 0 or 1 (binary classification: it binds or not the TF)
  - CTCF helps organizing the genome 3D structure (folding DNA into loops and domains that regulate gene activity)
